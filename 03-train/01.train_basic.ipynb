{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rby011/hf_study/blob/main/01_train_basic_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvuMjyWoY1NN"
      },
      "source": [
        "## 🎬 영화 리뷰 데이터로 BERT 모델 감성 분석하도록 훈련 시키기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3PseDeKY1NQ"
      },
      "source": [
        "### 📂 Load dataset\n",
        "\n",
        "- huggingface `datasets` package\n",
        "  ```\n",
        "    pip install datasets\n",
        "  ```\n",
        "\n",
        "- nsmc dataset\n",
        "  - https://huggingface.co/datasets/nsmc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers[torch] torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyUMwaCveLVF",
        "outputId": "b7f57fbe-7128-4424-9f7a-f3eba153c5a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3OBg2v4qY1NS",
        "outputId": "bb477645-9fd5-431c-9800-585d6fbd7089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "nsmc_dataset = load_dataset(\"nsmc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7nW7WTQpY1NV",
        "outputId": "bab0658b-159a-4eda-f9e1-29aa33f23c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'document', 'label'],\n",
              "        num_rows: 150000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'document', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nsmc_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6QSf2JGxY1NW",
        "outputId": "f5fd56f0-e2a3-4bcd-af2d-ff717dd75fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': Value(dtype='string', id=None),\n",
              " 'document': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['negative', 'positive'], id=None)}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'id': '9976970', 'document': '아 더빙.. 진짜 짜증나네요 목소리', 'label': 0}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#####################################################################################\n",
        "# DataSetDict 형태로 데이터 구조 , 내용 살펴보기\n",
        "#####################################################################################\n",
        "\n",
        "# document 는 리뷰 , label 은 리뷰의 긍정/부정 (positive / negative)\n",
        "display(nsmc_dataset['train'].features)\n",
        "\n",
        "# label 의 명칭은 'negative' , 'positive' 인데 여기서는 0, 1 로 표현되어 있음\n",
        "display(nsmc_dataset['train'][0])\n",
        "\n",
        "# label 명에 따른 label id 는 `label` feature 를 str2int 로 확인\n",
        "display(nsmc_dataset['train'].features['label'].str2int('positive'),\n",
        "        nsmc_dataset['train'].features['label'].str2int('negative'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7uwGS1JnY1NY",
        "outputId": "a505969a-beeb-4dd1-d66d-89672427e00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ff1719c-dbb7-4be3-98b9-a1327df31626\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ff1719c-dbb7-4be3-98b9-a1327df31626')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ff1719c-dbb7-4be3-98b9-a1327df31626 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ff1719c-dbb7-4be3-98b9-a1327df31626');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3c31df9f-440e-49a8-8d6f-7dff9b4cdad1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c31df9f-440e-49a8-8d6f-7dff9b4cdad1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3c31df9f-440e-49a8-8d6f-7dff9b4cdad1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(nsmc_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"3819312\",\n          \"6483659\",\n          \"10265843\"\n        ],\n        \"num_unique_values\": 5,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"\\ud760...\\ud3ec\\uc2a4\\ud130\\ubcf4\\uace0 \\ucd08\\ub529\\uc601\\ud654\\uc904....\\uc624\\ubc84\\uc5f0\\uae30\\uc870\\ucc28 \\uac00\\ubccd\\uc9c0 \\uc54a\\uad6c\\ub098\",\n          \"\\uc0ac\\uc774\\ubaac\\ud398\\uadf8\\uc758 \\uc775\\uc0b4\\uc2a4\\ub7f0 \\uc5f0\\uae30\\uac00 \\ub3cb\\ubcf4\\uc600\\ub358 \\uc601\\ud654!\\uc2a4\\ud30c\\uc774\\ub354\\ub9e8\\uc5d0\\uc11c \\ub299\\uc5b4\\ubcf4\\uc774\\uae30\\ub9cc \\ud588\\ub358 \\ucee4\\uc2a4\\ud2f4 \\ub358\\uc2a4\\ud2b8\\uac00 \\ub108\\ubb34\\ub098\\ub3c4 \\uc774\\ubed0\\ubcf4\\uc600\\ub2e4\",\n          \"\\ub108\\ubb34\\uc7ac\\ubc13\\uc5c8\\ub2e4\\uadf8\\ub798\\uc11c\\ubcf4\\ub294\\uac83\\uc744\\ucd94\\ucc9c\\ud55c\\ub2e4\"\n        ],\n        \"num_unique_values\": 5,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "label\n",
              "0    0.501153\n",
              "1    0.498847\n",
              "dtype: float64"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#####################################################################################\n",
        "# 판다스 데이터 프레임으로 변환해서 좀더 세밀히 살펴보기\n",
        "#####################################################################################\n",
        "\n",
        "# pandas 데이터 프레임으로 변환해서 자세히 살펴보기\n",
        "nsmc_df = nsmc_dataset['train'].to_pandas()\n",
        "display(nsmc_df.head())\n",
        "\n",
        "# (⚠️ 주의)분류 문제인 경우 label 불균등을 확인해보는 것이 중요\n",
        "# - 만일 label 이 매우 불균등한 상태라면 over sampling, under sampling 등 적용 필요\n",
        "# - negative 50.1% , positive 49.9%\n",
        "display(nsmc_df.groupby('label').apply(lambda x:len(x)/len(nsmc_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tTWlbxsCY1NZ",
        "outputId": "1d644b98-9d40-4131-d012-f80bfa9c993f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    150000.000000\n",
              "mean         35.203353\n",
              "std          29.532097\n",
              "min           0.000000\n",
              "25%          16.000000\n",
              "50%          27.000000\n",
              "75%          42.000000\n",
              "max         146.000000\n",
              "Name: review_length, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# 리뷰 문자열 길이 알아보기\n",
        "nsmc_df['review_length'] = nsmc_df['document'].str.len()\n",
        "nsmc_df['review_length'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdi0YStyY1NZ"
      },
      "source": [
        "#### 📂 preprocess\n",
        "\n",
        "자연어 문제의 기본은 `Tokenization` 으로 매우 중요. 예를 들어, `Tokenization` 의 방식에 따라 의미가 달라짐\n",
        "\n",
        "여기서는 BERT Multilangual Tokenizer 를 그대로 사용할 것 (BERT 모델에 들어 있는 것)\n",
        "\n",
        "`Auto` 붙은 것은 pretrained 모델을 갖고 올 때 그에 맞는 class 를 retrieval 해주는 것일 뿐\n",
        "\n",
        "모델은 https://huggingface.co/google-bert/bert-base-multilingual-cased  이 것 가지고 올 것임\n",
        "\n",
        "※ 한국어 Tokenizer , KoBERT 등 사용하는 것도 괜찮음\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_tmw4eUQY1Na",
        "outputId": "5cb074f5-871e-4ec5-d498-616c8d069597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "tok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jT_QOuxWY1Nb",
        "outputId": "93aa7263-8757-4826-b7d5-730f9c9c8361",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['청', '##춘', '영화', '##의', '최고', '##봉', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#####################################################################################\n",
        "# 문장 tokenize 해보기\n",
        "# - 한국어만으로 학습된 tokenizer 가 아닌 다국어로 학습된 것임에도 어느정도는 잘 함\n",
        "# - `##` 이 들어가지 않은 것은 단어가 시작되는 부분, 들어간 것은 이전 단어와 이어지는 것\n",
        "#####################################################################################\n",
        "seq = \"청춘 영화의 최고봉.\"\n",
        "\n",
        "tok.tokenize(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WEAqP06eY1Nc",
        "outputId": "258a08b9-4a88-4f5f-c42e-658a7e68134d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            " 'input_ids': [101, 9751, 97707, 42428, 10459, 83491, 118989, 119, 102],\n",
            " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "# input_ids 는 토큰을 vocabulary 에 포함된 토큰이 id 로 매핑한 결과\n",
        "# attention_mask 는 해당 토큰이 실제 데이터인지 여부를 마킹 (패팅 토큰 등은 0)\n",
        "# token_type_ids 는 문장을 구분하는데 사용됨.\n",
        "# - 예를 들어 premise, hypothesis 의 NLI (함의/모순/중립 판단) 하는 경우 두 개의 문장을 서로 구분하는 역할\n",
        "from pprint import pprint\n",
        "pprint(tok(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "81Dma2yPY1Nc",
        "outputId": "6431b4f2-168b-41a5-bb6c-6391a63a6048",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0, 0, 0, 0]],\n",
            " 'input_ids': [[101, 9751, 97707, 42428, 10459, 83491, 118989, 119, 102],\n",
            "               [101, 9751, 97707, 102, 0, 0, 0, 0, 0]],\n",
            " 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ],
      "source": [
        "# 패딩으로 채워진 두 번째 짧은 문장에서 패팅에 해당 하는 위치는 attention mask 가 0 임\n",
        "seq2 = '청춘'\n",
        "pprint(tok([seq, seq2], padding=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-zVy323Y1Nd"
      },
      "source": [
        "### 참고\n",
        "\n",
        "NLI Task 의 경우 아래 같은 코드를 통해 입력을 모델에게 줌 (예시임. GPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fjZL8il8Y1Ne"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "premise = \"The quick brown fox jumps over the lazy dog. It did not stop running.\"\n",
        "hypothesis = \"A fast animal is moving.\"\n",
        "\n",
        "input_text = \"[CLS] \" + premise + \" [SEP] \" + hypothesis + \" [SEP]\"\n",
        "tokenized_output = tokenizer.tokenize(input_text)\n",
        "\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokenized_output)\n",
        "\n",
        "# 🔑 [CLS] , [SEP] 이 포함된 모든 토큰에 대해 `1` 로 설정\n",
        "attention_mask = [1] * len(input_ids)\n",
        "\n",
        "# 🔑 premise 는 0 , hypothesis 는 1 로 설정\n",
        "sep_indices = [i for i, token in enumerate(tokenized_output) if token == \"[SEP]\"]\n",
        "token_type_ids = [0 if i <= sep_indices[0] else 1 for i in range(len(tokenized_output))]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ArVQiFa8Y1Ne"
      },
      "outputs": [],
      "source": [
        "# 로딩한 데이터(DataSetDict)의 문장을 최대 32 길이로 하고 짧은 경우 padding 하여 tokenize\n",
        "def tokenizer(data):\n",
        "    return tok(data['document'], max_length=32, truncation=True, padding='max_length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NjJXE4D4Y1Nf"
      },
      "outputs": [],
      "source": [
        "nsmc_dataset_toeknized = nsmc_dataset.map(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vYx7tClSY1Ng",
        "outputId": "4e65f74a-47c0-4e96-829e-81a3f9ea108f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'attention_mask': [1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    1,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0],\n",
            " 'document': '아 더빙.. 진짜 짜증나네요 목소리',\n",
            " 'id': '9976970',\n",
            " 'input_ids': [101,\n",
            "               9519,\n",
            "               9074,\n",
            "               119005,\n",
            "               119,\n",
            "               119,\n",
            "               9708,\n",
            "               119235,\n",
            "               9715,\n",
            "               119230,\n",
            "               16439,\n",
            "               77884,\n",
            "               48549,\n",
            "               9284,\n",
            "               22333,\n",
            "               12692,\n",
            "               102,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0,\n",
            "               0],\n",
            " 'label': 0,\n",
            " 'token_type_ids': [0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0,\n",
            "                    0]}\n"
          ]
        }
      ],
      "source": [
        "# padding 정확하게 들어간건지 확인해야 함\n",
        "pprint(nsmc_dataset_toeknized['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oejT-JtY1Nh"
      },
      "source": [
        "### 🔄 load model for finetuing\n",
        "\n",
        "`bart-base` 모델을 기반으로 각 task 별로 적절한 layer 를 덧붙이는 것이 필요함.\n",
        "\n",
        "예를 들면, 문장 분류 작업을 위해서는 dense layer 와 softmax 를 붙여서 사용을 해야 하고,\n",
        "\n",
        "질의 응답 같은 경우는 base 모델이 출력하는 토큰 중에서 어디서 부터 어디까지가 답변인지를 식별하는 layer 를 붙여두어야 함\n",
        "\n",
        "transformer 패키지는 각각에 대해서 사전 정의된 layer 를 붙여서 모델을 로딩하도록 할 수 있도록 하고 있고\n",
        "\n",
        "`BertQuestionAnswering`, `BertSequenceClassification` 등의 클래스가 이에 해당함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LjTWyme-Y1Nh",
        "outputId": "4598f4b9-6a98-4d5a-b4ad-0a6db0740190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# device 정의\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainig 에 필요한 설정 정의\n",
        "num_train_epochs = 2\n",
        "learning_rate = 2e-7\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "a6hCO73Wa2Nk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mNKnME30Y1Ni",
        "outputId": "f5e6dd03-22bf-47f7-c2e0-f52e2c0082f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# task 에 적합하도록 layer 를 붙여서 model 로딩\n",
        "#\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💪 finetune with pytorch\n",
        "\n",
        "- toeknizer 와 연계해서 dataloader 만들기"
      ],
      "metadata": {
        "id": "IeCpI1sSd3A-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PFD8NfnYY1Ni"
      },
      "outputs": [],
      "source": [
        "# 최대 토큰 길이 32 로 하고 padding 추가해서 토큰화\n",
        "def tokenizer(data):\n",
        "    return tok(data['document'], max_length=32, truncation=True, padding='max_length')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 데이터를 대상으로 토큰화\n",
        "nsmc_dataset_toeknized = nsmc_dataset.map(tokenizer)"
      ],
      "metadata": {
        "id": "bnCfKcBKaZis"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 데이터에 대해서 토큰화하여 dataloader 로 만들기\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tr_ds = nsmc_dataset_toeknized['train'].remove_columns(['id', 'document'])\n",
        "tr_ds.set_format(type='torch')\n",
        "tr_dl = DataLoader(tr_ds, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "LJj8OrR9abQh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(tr_dl))"
      ],
      "metadata": {
        "id": "A7UIVlRManh_",
        "outputId": "56ed906a-acce-4bc8-aacd-69e16fc2ead0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
              "         0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
              "         0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
              "         0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
              "         1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
              "         0, 0, 1, 1, 0, 1, 0, 1]),\n",
              " 'input_ids': tensor([[  101,  9519,  9074,  ...,     0,     0,     0],\n",
              "         [  101,   100,   119,  ..., 16439,   102,     0],\n",
              "         [  101,   100,   102,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  101,  9358, 12508,  ...,     0,     0,     0],\n",
              "         [  101,  9519, 25503,  ...,     0,     0,     0],\n",
              "         [  101, 10150, 10954,  ...,  9568, 12310,   102]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 1, 1, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터를 토큰화해서 dataloader 만들기\n",
        "val_ds = nsmc_dataset_toeknized['test'].remove_columns(['id', 'document'])\n",
        "val_ds.set_format(type='torch')\n",
        "val_dl = DataLoader(val_ds, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "6WQp1uHIa7_p"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(val_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEzijlkLe6t3",
        "outputId": "89f11801-473f-4593-da45-010b2feaa5f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "         0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
              "         1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
              "         0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
              "         1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "         0, 0, 1, 1, 0, 1, 0, 1]),\n",
              " 'input_ids': tensor([[   101,   8911,    100,  ...,      0,      0,      0],\n",
              "         [   101,    144,  11490,  ...,      0,      0,      0],\n",
              "         [   101,   9303,  21711,  ...,      0,      0,      0],\n",
              "         ...,\n",
              "         [   101,  25805, 118990,  ...,  35506,  16439,    102],\n",
              "         [   101,  80956,   9511,  ...,      0,      0,      0],\n",
              "         [   101,   9670,  89523,  ...,    119,    119,    102]]),\n",
              " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0],\n",
              "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         ...,\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 0, 0, 0],\n",
              "         [1, 1, 1,  ..., 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🏋️학습코드 살펴보기\n",
        "\n",
        "<font color='yellow'> ★ loss 선택하기 </font>\n",
        "\n",
        "- 이진 분류의 문제를 풀기 위해 CrossEntropy 함수를 선택하였고 그 입력으로 양성으로 예측한 확률과 양성/음성 라벨 값(1/0) 을 넣었음\n",
        "\n",
        "- 모델 예측 결과인 `pred` 는 다음과 같은 형식임\n",
        "\n",
        "  ```python\n",
        "   SequenceClassifierOutput(\n",
        "      loss=None,\n",
        "      logits=tensor([[ 0.1504, -0.0221],\n",
        "        [ 0.0619, -0.0455],\n",
        "        [ 0.0928, -0.0023],\n",
        "        [ 0.1067, -0.0332],\n",
        "        [ 0.0868, -0.0205],\n",
        "  ```\n",
        "- 위 pred 의 logits 중 1번째 것만을 취하기 위해 t() 를 한 것"
      ],
      "metadata": {
        "id": "d09g0Wj3iwlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Accuracy 함수 정의\n",
        "def acc(pred , label):\n",
        "  pred = torch.round(pred.squeeze())\n",
        "  return torch.sum(pred == label.squeeze()).item()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# for each EPOCH\n",
        "for epoch in range(num_train_epochs):\n",
        "  train_lossses = []\n",
        "  train_acc = 0.0\n",
        "  model.train()\n",
        "  # for each BATCH in an epoch\n",
        "  for step, batch in enumerate(tqdm(tr_dl)):\n",
        "    label = batch['label'].to(device)\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    pred = model(input_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(torch.sigmoid(pred.logits.t()[1]), label.float())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_lossses.append(loss.item())\n",
        "    train_acc = train_acc + acc(pred.logits.argmax(dim=1), label)\n",
        "\n",
        "  # one epoch train\n",
        "  train_loss = np.mean(train_lossses)\n",
        "  train_acc = train_acc / len(tr_ds)\n",
        "  print(f'# epoch: {epoch+1}, train loss: {train_loss:.4f}, train acc: {train_acc:.4f}')\n",
        "\n",
        "  # eval after one epoch\n",
        "  model.eval()\n",
        "  val_lossses = []\n",
        "  val_acc = 0.0\n",
        "  # for each BATCH in an epoch\n",
        "  for step, batch in enumerate(tqdm(val_dl)):\n",
        "    label = batch['label'].to(device)\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "\n",
        "    pred = model(input_ids, attention_mask, token_type_ids)\n",
        "    loss = criterion(torch.sigmoid(pred.logits.t()[1]), label.float())\n",
        "\n",
        "    val_lossses.append(loss.item())\n",
        "    val_acc =  val_acc + acc(pred.logits.argmax(dim=1), label)\n",
        "\n",
        "  val_loss = np.mean(val_lossses)\n",
        "  val_acc = val_acc / len(val_ds)\n",
        "\n",
        "  print(f'# epoch: {epoch+1}, validation loss: {val_loss:.4f}, train acc: {val_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfZ0nv_9fIba",
        "outputId": "f1fb5fcf-8db8-45eb-a1d0-456f321b9e05"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [04:09<00:00,  4.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# epoch: 1, train loss: 307.7467, train acc: 0.6072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:27<00:00, 14.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# epoch: 1, validation loss: 307.0408, train acc: 0.6990\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1172/1172 [04:07<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# epoch: 2, train loss: 303.5219, train acc: 0.7133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 391/391 [00:27<00:00, 13.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# epoch: 2, validation loss: 304.6934, train acc: 0.7328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 출력 살펴보기\n",
        "# - SequenceClassifierOutput 클래스이고 loss, logits 속성이 있고\n",
        "# - logits 속성은 tensor 형태로 양성/음성에 대한 logit 을 줌\n",
        "\n",
        "# 모델 GPU 에 넣기\n",
        "# model.to(device)\n",
        "\n",
        "# 모델에 줄 입력 값들(첫 배치) GPU 에 넣기\n",
        "batch1 = next(iter(tr_dl))\n",
        "label = batch1['label'].to(device)\n",
        "input_ids = batch1['input_ids'].to(device)\n",
        "attention_mask = batch1['attention_mask'].to(device)\n",
        "token_type_ids = batch1['token_type_ids'].to(device)\n",
        "\n",
        "# 모델에 입력값 넣기\n",
        "pred = model(input_ids, attention_mask, token_type_ids)\n",
        "\n",
        "display(pred.logits.t())\n",
        "\n",
        "# 첫 입력, 정답, 예측 결과 살펴보기\n",
        "tok.decode(input_ids[0]) , label[0], torch.sigmoid(pred.logits.t()[1])[0]"
      ],
      "metadata": {
        "id": "GyAnIAlCf1ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💪 finetune with 허깅페이스"
      ],
      "metadata": {
        "id": "Tk3PfvERtxcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "output_dir = './trainer_test'\n",
        "\n",
        "training_args = TrainingArguments(output_dir=output_dir,\n",
        "                                  num_train_epochs=num_train_epochs,\n",
        "                                  learning_rate=learning_rate,\n",
        "                                  per_device_train_batch_size=batch_size,\n",
        "                                  per_device_eval_batch_size=batch_size,\n",
        "                                  evaluation_strategy='epoch',\n",
        "                                  logging_steps=len(nsmc_dataset['train'])//batch_size,\n",
        "                                  save_strategy='epoch',\n",
        "                                  fp16=True,\n",
        "                                  push_to_hub=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "oRdjCV3Yjplj",
        "outputId": "1d017885-048c-465c-e8be-7aa19fa311b4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-786d2dc4f9d2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./trainer_test'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m training_args = TrainingArguments(output_dir=output_dir,\n\u001b[0m\u001b[1;32m      6\u001b[0m                                   \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                   \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memo...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"npu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1885\u001b[0m         \"\"\"\n\u001b[1;32m   1886\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1887\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.20.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   1788\u001b[0m                     \u001b[0;34m\"Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                 )\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUbeUkcavNo3",
        "outputId": "65906979-4564-4d9a-db8f-9e183c1cc636"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: accelerate\n",
            "Version: 0.27.2\n",
            "Summary: Accelerate\n",
            "Home-page: https://github.com/huggingface/accelerate\n",
            "Author: The HuggingFace team\n",
            "Author-email: sylvain@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3tGgUgXxuzv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}